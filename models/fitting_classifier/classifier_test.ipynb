{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *\n",
    "from options import *\n",
    "\n",
    "import torch.utils.data\n",
    "import os.path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import operator\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "class BaseOptions():\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self):    \n",
    "        # experiment specifics\n",
    "        self.parser.add_argument('--name', type=str, default='label2city', help='name of the experiment. It decides where to store samples and models')        \n",
    "        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')                       \n",
    "        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "        self.parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization')        \n",
    "        self.parser.add_argument('--use_dropout', action='store_true', help='use dropout for the generator')\n",
    "\n",
    "        # input/output sizes       \n",
    "        self.parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n",
    "        self.parser.add_argument('--loadSize', type=int, default=1024, help='scale images to this size')\n",
    "        self.parser.add_argument('--fineSize', type=int, default=512, help='then crop to this size')\n",
    "        self.parser.add_argument('--label_nc', type=int, default=35, help='# of input image channels')\n",
    "        self.parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels')\n",
    "        self.parser.add_argument('--use_PIL', action='store_true', help='if true, uses the PIL library for data loading, openCV by default')\n",
    "        self.parser.add_argument('--input_mask_fill', '--fill', dest='fill', type=str, default='W&B', help='fill to use in the input images for the artificialiy created masks, \\\n",
    "                                  options: \"B&W\" (salt&peper - default) | \"W\" (white) | \"B\" (black) | \"G\" (grey)')\n",
    "      \n",
    "        # for setting inputs\n",
    "        self.parser.add_argument('--dataroot', type=str, default='/blanca/training_datasets/pix2pix/images_target_clean_classified') \n",
    "        self.parser.add_argument('--dataset_list', action='store', type=str, nargs='*', default='images_target_clean_classified video_target_clean_classified') \n",
    "        self.parser.add_argument('--resize_or_crop', type=str, default='scale_width', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]')\n",
    "        self.parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')        \n",
    "        self.parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data argumentation') \n",
    "        self.parser.add_argument('--nThreads', default=2, type=int, help='# threads for loading data')                \n",
    "        self.parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "\n",
    "        # for displays\n",
    "        self.parser.add_argument('--display_winsize', type=int, default=512,  help='display window size')\n",
    "        self.parser.add_argument('--tf_log', action='store_true', help='if specified, use tensorboard logging. Requires tensorflow installed')\n",
    "        \n",
    "        ## visdom\n",
    "        self.parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')\n",
    "        self.parser.add_argument('--display_port', type=int, default=8097, help='visdom port of the web display')\n",
    "\n",
    "        # for generator\n",
    "        self.parser.add_argument('--netG', type=str, default='global', help='selects model to use for netG')\n",
    "        self.parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
    "        self.parser.add_argument('--n_downsample_global', type=int, default=4, help='number of downsampling layers in netG') \n",
    "        self.parser.add_argument('--n_blocks_global', type=int, default=9, help='number of residual blocks in the global generator network')\n",
    "        self.parser.add_argument('--n_blocks_local', type=int, default=3, help='number of residual blocks in the local enhancer network')\n",
    "        self.parser.add_argument('--n_local_enhancers', type=int, default=1, help='number of local enhancers to use')        \n",
    "        self.parser.add_argument('--niter_fix_global', type=int, default=0, help='number of epochs that we only train the outmost local enhancer')        \n",
    "\n",
    "        # for instance-wise features\n",
    "        self.parser.add_argument('--no_instance', action='store_true', help='if specified, do *not* add instance map as input')        \n",
    "        self.parser.add_argument('--instance_feat', action='store_true', help='if specified, add encoded instance features as input')\n",
    "        self.parser.add_argument('--label_feat', action='store_true', help='if specified, add encoded label features as input')        \n",
    "        self.parser.add_argument('--feat_num', type=int, default=3, help='vector length for encoded features')        \n",
    "        self.parser.add_argument('--load_features', action='store_true', help='if specified, load precomputed feature maps')\n",
    "        self.parser.add_argument('--n_downsample_E', type=int, default=4, help='# of downsampling layers in encoder') \n",
    "        self.parser.add_argument('--nef', type=int, default=16, help='# of encoder filters in the first conv layer')        \n",
    "        self.parser.add_argument('--n_clusters', type=int, default=10, help='number of clusters for features')\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "    def parse(self, save=True):\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.opt = self.parser.parse_args()\n",
    "        self.opt.isTrain = self.isTrain   # train or test\n",
    "\n",
    "        str_ids = self.opt.gpu_ids.split(',')\n",
    "        self.opt.gpu_ids = []\n",
    "        for str_id in str_ids:\n",
    "            id = int(str_id)\n",
    "            if id >= 0: self.opt.gpu_ids.append(id)\n",
    "        \n",
    "        # set gpu ids\n",
    "        if len(self.opt.gpu_ids) > 0: torch.cuda.set_device(self.opt.gpu_ids[0])\n",
    "\n",
    "        args = vars(self.opt)\n",
    "\n",
    "        print('------------ Options -------------')\n",
    "        for k, v in sorted(args.items()): print('%s: %s' % (str(k), str(v)))\n",
    "        print('-------------- End ----------------')\n",
    "\n",
    "        # save to the disk        \n",
    "        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n",
    "        util.mkdirs(expr_dir)\n",
    "        if save and not self.opt.continue_train:\n",
    "            file_name = os.path.join(expr_dir, 'opt.txt')\n",
    "            with open(file_name, 'wt') as opt_file:\n",
    "                opt_file.write('------------ Options -------------\\n')\n",
    "                for k, v in sorted(args.items()): opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
    "                opt_file.write('-------------- End ----------------\\n')\n",
    "        return self.opt\n",
    "\n",
    "class TrainOptions(BaseOptions):\n",
    "    def initialize(self):\n",
    "        BaseOptions.initialize(self)\n",
    "        # for displays\n",
    "        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n",
    "        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
    "        self.parser.add_argument('--save_latest_freq', type=int, default=10000, help='frequency of saving the latest results')\n",
    "        self.parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')        \n",
    "        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
    "        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n",
    "        ## visdom:\n",
    "        self.parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n",
    "\n",
    "        # for training\n",
    "        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        self.parser.add_argument('--load_pretrain', type=str, default='', help='load the pretrained model from the specified location')\n",
    "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--niter', type=int, default=50, help='# of iter at starting learning rate')\n",
    "        self.parser.add_argument('--niter_decay', type=int, default=50, help='# of iter to linearly decay learning rate to zero')\n",
    "        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
    "        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "\n",
    "        # for discriminators        \n",
    "        self.parser.add_argument('--num_D', type=int, default=2, help='number of discriminators to use')\n",
    "        self.parser.add_argument('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\n",
    "        self.parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')    \n",
    "        self.parser.add_argument('--lambda_feat', type=float, default=10.0, help='weight for feature matching loss')                \n",
    "        self.parser.add_argument('--no_ganFeat_loss', action='store_true', help='if specified, do *not* use discriminator feature matching loss')\n",
    "        self.parser.add_argument('--no_vgg_loss', action='store_true', help='if specified, do *not* use VGG feature matching loss')        \n",
    "        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n",
    "        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n",
    "        \n",
    "        # for the generator:\n",
    "        self.parser.add_argument('--lambda_L1', type=float, default=0, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--lambda_G_cos1', type=float, default=0, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--lambda_G_cos2', type=float, default=0, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--lambda_G_cos1_z', type=float, default=0, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--lambda_G_cos2_z', type=float, default=0, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--lambda_G_KL_fake', type=float, default=0, help='weight for the KL loss')\n",
    "        self.parser.add_argument('--lambda_E_KL_real', type=float, default=0, help='weight for the KL_real loss')\n",
    "        self.parser.add_argument('--lambda_E_KL_fake', type=float, default=0, help='weight for the KL_fake loss')\n",
    "        \n",
    "        self.parser.add_argument('--lambda_G_class', type=bool, default=10, help='weight for the classsification loss')\n",
    "        \n",
    "        # for the classifier\n",
    "        self.parser.add_argument('--class_nc', type=int, default=4, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--n_layers_C', type=int, default=3, help='weight for the L1 loss')\n",
    "        self.parser.add_argument('--num_C', type=int, default=1, help='weight for the L1 loss')\n",
    "        \n",
    "        self.isTrain = True\n",
    "\n",
    "def read_image_OpenCV(path, opt, is_pair=False, target_size=None):\n",
    "        im = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        if is_pair: im = cv2.resize(im, (opt.loadSize * 2, opt.loadSize))\n",
    "        elif target_size: im = cv2.resize(im, target_size)\n",
    "        else: im = cv2.resize(im, (opt.loadSize, opt.loadSize))\n",
    "        return im\n",
    "\n",
    "def fill_gaps(im, opt,\n",
    "              fill_input_with=None,\n",
    "              add_artificial=False,\n",
    "              only_artificial=False,\n",
    "              cm_p = '/blanca/resources/contour_mask/contour_mask.png'):\n",
    "    \n",
    "    if not fill_input_with: fill_input_with = opt.fill\n",
    "    \n",
    "    # creating fills for filling gaps\n",
    "    n_fill_WB = cv2.randu(np.zeros(im.shape[:2]), 0, 255)\n",
    "    n_fill_B = np.zeros(im.shape[:2]) * 255\n",
    "    n_fill_W = np.ones(im.shape[:2]) * 255\n",
    "    n_fill_G = np.ones(im.shape[:2]) * 127\n",
    "    \n",
    "    which_fill = dict(zip(['W', 'B', 'W&B', 'G'], [n_fill_W, n_fill_B, n_fill_WB, n_fill_G]))\n",
    "\n",
    "    # read contour template mask\n",
    "    # im_cm = cv2.imread(cm_p, cv2.IMREAD_UNCHANGED)\n",
    "    im_cm = read_image_OpenCV(cm_p, opt, is_pair=False)\n",
    "    assert im_cm is not None, 'Make sure there is a \"contour_mask.png\" file in this folder'\n",
    "    mask = im_cm == 255 # contour mask: internal \n",
    "    \n",
    "    new_alpha = im[:,:,3] != 0\n",
    "    new_alpha[im_cm == 0] = 1 # ensuring we exclude corners what != txt. map\n",
    "    im[:,:,3] = new_alpha * 255 # applying\n",
    "    \n",
    "    if not only_artificial:\n",
    "        for i in range(im.shape[2] - 1):\n",
    "            if fill_input_with=='average':\n",
    "                mask_average = im[:,:,3] != 0\n",
    "                # print('Filling gaps with %f instead of %f' %(np.mean(im[:,:,0]), np.mean(im[:,:,0][mask_average])))\n",
    "                im[:,:,i][~new_alpha] = (np.ones(im.shape[:2]) * np.mean(im[:,:,i][mask_average]))[~new_alpha]    \n",
    "            else: im[:,:,i][~new_alpha] = which_fill[fill_input_with][~new_alpha]\n",
    "    \n",
    "    if add_artificial and opt.phase != 'test':\n",
    "        \n",
    "        ## SELECT AN OCCLUSSION AND APPLY TO THE IMAGE\n",
    "        gpath = Path('/blanca')\n",
    "        rpath = gpath / 'resources/db_occlusions'\n",
    "        rpath = [rpath]\n",
    "        rpaths = reduce(operator.add, \n",
    "                      [list(j.glob('*')) for j in rpath],\n",
    "                      [])\n",
    "        \n",
    "        random_idx = random.sample(range(len(rpaths)), len(rpaths))\n",
    "        ix = random.sample(range(len(rpaths)), 1)[0]\n",
    "\n",
    "        imr = read_image_OpenCV(str(rpaths[ix]), opt, is_pair=False)\n",
    "        alpha_artifitial = imr\n",
    "        alpha_artifitial[alpha_artifitial != 0] = 1\n",
    "        im[:,:,3][im[:,:,3] != 0] = 1\n",
    "        new_alpha_artifitial = alpha_artifitial * im[:,:,3]\n",
    "        # setting the new alpha channel\n",
    "        im[:,:,3] = new_alpha_artifitial * 255\n",
    "        \n",
    "        # filling\n",
    "        for i in range(im.shape[2] - 1):\n",
    "            im[:,:,i][new_alpha_artifitial == 0] = which_fill[fill_input_with][new_alpha_artifitial == 0]\n",
    "    \n",
    "    return im\n",
    "\n",
    "def reduce_and_shuffle_dict_values_nested1level(d):\n",
    "    \n",
    "    flat = reduce(operator.add,\n",
    "                      [reduce(operator.add, i.values(), []) for i in list(d.values())], \n",
    "                      [])\n",
    "\n",
    "    [random.shuffle(flat) for i in range(int(1e2))]\n",
    "\n",
    "    return flat\n",
    "\n",
    "def create_dataset_from_dir2subdir(dir, class_label=None, nitems=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of paths from a nesteed dir with two levels, selecting nitems from each dir of the last level \n",
    "    \"\"\"\n",
    "    \n",
    "    EXT_RECURSIVE = ['**/*.jpg', '**/*.JPG', '**/*.png', '**/*.ppm']\n",
    "    from collections import OrderedDict\n",
    "        \n",
    "    path = Path(dir)\n",
    "    id_names = [i.parts[-1] for i in list(path.glob('*')) if os.path.isdir(i)]\n",
    "    \n",
    "    n_items_per_last_level = nitems\n",
    "\n",
    "    data_dict = OrderedDict({i: {} for i in sorted(id_names)})\n",
    "    data_dict_nitems = OrderedDict({i: {} for i in sorted(id_names)})\n",
    "\n",
    "    # INITIALISE\n",
    "    for i in id_names:\n",
    "        for j in os.listdir(path/i):\n",
    "            data_dict[i][j] = None\n",
    "\n",
    "    # FILLING\n",
    "    import random\n",
    "    random.seed()\n",
    "\n",
    "    for i in data_dict.keys():\n",
    "        for j in data_dict[i].keys():\n",
    "            txt_pl = reduce(\n",
    "                  operator.add,\n",
    "                  [list((path/i/j).glob('**/*.isomap.png'))],\n",
    "                  [])\n",
    "\n",
    "            # DICT WITH ALL PATHS\n",
    "            data_dict[i][j] = txt_pl\n",
    "\n",
    "            # DICT WITH MAX(N) PATHS\n",
    "            random_idx = random.sample(range(len(txt_pl)), min(len(txt_pl), n_items_per_last_level))\n",
    "            txt_pl_nitems = [str(txt_pl[i]) for i in random_idx]\n",
    "\n",
    "            data_dict_nitems[i][j] = txt_pl_nitems\n",
    "\n",
    "    print('Total found IDs in path %s: %d' %(path, len(data_dict_nitems)), '.. and selected %d per ID' %n_items_per_last_level)\n",
    "    \n",
    "    data_list_n_shuffled = reduce_and_shuffle_dict_values_nested1level(data_dict_nitems)\n",
    "    data_list_n_shuffled_labeled = [(i, class_label) for i in data_list_n_shuffled]\n",
    "    return data_list_n_shuffled_labeled\n",
    "\n",
    "def create_dataset_fromIDsubfolders(path, nitems=None):\n",
    "    assert os.path.isdir(path), '%s is not a valid directory' % path\n",
    "    images = create_dataset_from_dir2subdir(path, nitems)\n",
    "    return images\n",
    "\n",
    "def create_dataset_fromIDsubfolders_withLabel(path, classes=['bad_fit', 'good_fit'], nitems=None):\n",
    "\n",
    "    assert os.path.isdir(path), '%s is not a valid directory' % path\n",
    "    \n",
    "    path = Path(path)\n",
    "    class_paths = [i for i in path.glob('*') if os.path.isdir(i)]\n",
    "    # there are no pairs for videos (f.d.m.)\n",
    "    path_list = []\n",
    "    for i in class_paths:\n",
    "            class_label = classes.index(i.parts[-1])\n",
    "            path_list += create_dataset_from_dir2subdir(i, class_label, nitems)\n",
    "    print('shuffling...', end='')\n",
    "    random.seed(1984)\n",
    "    [random.shuffle(path_list) for i in range(int(1e2))]\n",
    "    print('done')\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "def create_dataset_withLabel(path, classes=['bad_fit', 'good_fit'], return_pairs=None):\n",
    "        path = Path(path)\n",
    "        class_paths = [i for i in path.glob('*') if os.path.isdir(i)]\n",
    "        pairs_path_list = []\n",
    "        for i in class_paths:\n",
    "                class_label = classes.index(i.parts[-1])\n",
    "                pairs_path_list += create_dataset(i, class_label)\n",
    "        \n",
    "        print('shuffling...', end='')\n",
    "        random.seed(1984)\n",
    "        [random.shuffle(pairs_path_list) for i in range(int(1e2))]\n",
    "        print('done')\n",
    "        \n",
    "        return pairs_path_list\n",
    "        \n",
    "def create_dataset(path, class_label=None, return_pairs=None):\n",
    "        path = Path(path)\n",
    "        ims_path_list = path.glob('*_m.png')\n",
    "        \n",
    "        pairs_path_list = []\n",
    "        for i in ims_path_list:\n",
    "                pair_name = i.parts[-1].split('_m.png')[0] + '.png'\n",
    "                pair_path = str(list(path.glob(pair_name))[0])\n",
    "                pair_path_mirror = str(i)\n",
    "                pairs_path_list.append([(pair_path, class_label), (pair_path_mirror, class_label)])\n",
    "\n",
    "        print('shuffling...', end='')\n",
    "        random.seed(1984)\n",
    "        [random.shuffle(pairs_path_list) for i in range(int(1e2))]\n",
    "        print('done')\n",
    "        \n",
    "        if return_pairs==None: pairs_path_list = reduce(operator.add, pairs_path_list, [])\n",
    "        return pairs_path_list\n",
    "\n",
    "\n",
    "def apply_data_transforms(im, which, opt, nchannels):\n",
    "        \n",
    "    transform_list = []\n",
    "    if which == 'target':\n",
    "        transform_list += [\n",
    "                transforms.Lambda(lambda x: fill_gaps(x, opt, fill_input_with='average')),\n",
    "                transforms.Lambda(lambda x: x[:, :, :nchannels]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "                \n",
    "    elif which == 'targetXC':\n",
    "         transform_list += [\n",
    "                transforms.Lambda(lambda x: x[:, :, :nchannels]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                ]\n",
    "    \n",
    "    elif which == 'input':        \n",
    "        transform_list += [\n",
    "            transforms.Lambda(lambda x: x.copy()),\n",
    "            transforms.Lambda(lambda x: fill_gaps(x, opt, add_artificial=opt.isTrain)),\n",
    "            transforms.Lambda(lambda x: x[:, :, :nchannels]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]                                    \n",
    "                                         \n",
    "    return transforms.Compose(transform_list)(im)\n",
    "\n",
    "\n",
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(BaseDataset, self).__init__()\n",
    "\n",
    "    def name(self):\n",
    "        return 'BaseDataset'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AlignedDataset(BaseDataset):\n",
    "        \n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.root = opt.dataroot    \n",
    "        self.dataset_list = opt.dataset_list\n",
    "\n",
    "        self.target_paths = []\n",
    "        if self.opt.isTrain:\n",
    "            self.target_paths += create_dataset_withLabel(os.path.join(self.root, self.dataset_list[0])); print(len(self.target_paths))\n",
    "            self.target_paths += create_dataset_fromIDsubfolders_withLabel(os.path.join(self.root, self.dataset_list[1]), nitems=2); print(len(self.target_paths))\n",
    "        else:\n",
    "            self.target_paths += create_dataset(os.path.join(self.root, self.dataset_list[0]))\n",
    "  \n",
    "        self.dataset_size = len(self.target_paths) \n",
    "      \n",
    "    def __getitem__(self, index):                             \n",
    "        \n",
    "        target_tensor = target_label_tensor = inst_tensor = feat_tensor = 0\n",
    "        \n",
    "        input_nc = self.opt.label_nc if self.opt.label_nc != 0 else 3\n",
    "        output_nc = self.opt.output_nc\n",
    "        \n",
    "        # read target image\n",
    "        target_path = self.target_paths[index][0]\n",
    "        target_label = self.target_paths[index][1]\n",
    "        target_im = read_image_OpenCV(target_path, self.opt)\n",
    "        target_im_resized = read_image_OpenCV(target_path, self.opt, target_size=(224, 224))\n",
    "        \n",
    "        # create input tensor first\n",
    "        # if self.opt.isTrain:\n",
    "        input_tensor = apply_data_transforms(target_im, 'input', self.opt, input_nc)\n",
    "        \n",
    "        # create output tensor\n",
    "        if self.opt.isTrain: \n",
    "                target_tensor = apply_data_transforms(target_im, 'target', self.opt, output_nc)\n",
    "                target4C_tensor = apply_data_transforms(target_im_resized, 'targetXC', self.opt, nchannels=4)\n",
    "        \n",
    "        # if target_label:\n",
    "        target_label_tensor = torch.FloatTensor([target_label])\n",
    "        \n",
    "        input_dict = {'input': input_tensor, 'inst': inst_tensor, \n",
    "                      'target': target_tensor, 'feat': feat_tensor, \n",
    "                      'target4C': target4C_tensor,\n",
    "                      'path': target_path, 'label': target_label_tensor}\n",
    "\n",
    "        return input_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'AlignedDataset'\n",
    "\n",
    "def CreateDataset(opt):\n",
    "    dataset = None\n",
    "    dataset = AlignedDataset()\n",
    "    dataset.initialize(opt)\n",
    "    print(\"dataset [%s] was created\" % (dataset.name()))\n",
    "    return dataset\n",
    "\n",
    "class BaseDataLoader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        pass\n",
    "\n",
    "    def load_data():\n",
    "        return None\n",
    "\n",
    "class CustomDatasetDataLoader(BaseDataLoader):\n",
    "    def name(self):\n",
    "        return 'CustomDatasetDataLoader'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        BaseDataLoader.initialize(self, opt)\n",
    "        self.dataset = CreateDataset(opt)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=opt.batchSize,\n",
    "            shuffle=not opt.serial_batches,\n",
    "            num_workers=int(opt.nThreads))\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.dataloader\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.opt.max_dataset_size)\n",
    "\n",
    "def CreateDataLoader(opt):\n",
    "    data_loader = CustomDatasetDataLoader()\n",
    "    print(data_loader.name())\n",
    "    data_loader.initialize(opt)\n",
    "    return data_loader    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class options():\n",
    "    def __init__(self):\n",
    "        self.dataroot='/blanca/training_datasets/pix2pix/'\n",
    "        self.dataset_list=['images_target_clean_classified', 'video_target_clean_classified']\n",
    "        self.isTrain=True\n",
    "        self.batchSize=1\n",
    "        self.loadSize=128\n",
    "        self.fineSize=128\n",
    "        self.label_nc=35\n",
    "        self.output_nc=3\n",
    "        self.fill='W&B'\n",
    "        self.serial_batches=False\n",
    "        self.nThreads=16\n",
    "        self.max_dataset_size=float(\"inf\")\n",
    "        # for training\n",
    "        self.continue_train=False\n",
    "        self.which_epoch='latest'\n",
    "        self.phase='train'\n",
    "        self.niter=50\n",
    "        self.niter_decay=50\n",
    "        self.beta1=0.5\n",
    "        self.lr=0.0002\n",
    "        \n",
    "        # for the classifier\n",
    "        self.class_nc=4\n",
    "        self.n_layers_C=3\n",
    "        self.num_C=1\n",
    "        \n",
    "opt = options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "print('#training images = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e0fd7ad59708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_classifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepoch_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mniter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mniter_decay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-53540ced165b>\u001b[0m in \u001b[0;36mset_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseNetMulti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_nc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpretrained_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fitting_classifier/checkpoints_densenet_test/test_clean/model_best_c4_0.742152_0.018293_1519316.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "model = set_classifier_model()\n",
    "\n",
    "total_steps = (start_epoch-1) * dataset_size + epoch_iter\n",
    "for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    if epoch != start_epoch: epoch_iter = epoch_iter % dataset_size\n",
    "    for i, data in enumerate(dataset, start=epoch_iter):\n",
    "        iter_start_time = time.time()\n",
    "        total_steps += opt.batchSize\n",
    "        epoch_iter += opt.batchSize\n",
    "\n",
    "        # whether to collect output images\n",
    "        save_fake = total_steps % opt.display_freq == 0\n",
    "\n",
    "        ############## Forward Pass ######################\n",
    "#         losses, generated = model(Variable(data['input']), Variable(data['inst']), \n",
    "#                                   Variable(data['target']), Variable(data['feat']), \n",
    "#                                   Variable(data['target4C']),\n",
    "#                                   Variable(data['label']),\n",
    "#                                   infer=save_fake)\n",
    "\n",
    "        # sum per device losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n",
      "935\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/blanca/utils')\n",
    "import libraries\n",
    "from utils_global import *\n",
    "\n",
    "path_g = '/blanca/training_datasets/pix2pix/images_target_clean_classified/good_fit'\n",
    "path_b = '/blanca/training_datasets/pix2pix/images_target_clean_classified/bad_fit'\n",
    "path_g = Path(path_g)\n",
    "path_b = Path(path_b)\n",
    "path_glist = list(path_g.glob('*.png')); print(len(path_glist))\n",
    "path_blist = list(path_b.glob('*.png')); print(len(path_blist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier_model():\n",
    "        import networks\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        \n",
    "        model = networks.DenseNetMulti(nchannels=opt.class_nc)\n",
    "        model.initialize(opt)\n",
    "        if use_gpu: model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count()))).cuda()\n",
    "\n",
    "        pretrained_1 = 'fitting_classifier/checkpoints_densenet_test/test_clean/model_best_c4_0.742152_0.018293_1519316.pt'      \n",
    "        pretrained_2 = 'fitting_classifier/checkpoints_densenet_test/test_clean/model_best_c4_0.829596_0.023838_1519321.pt'   \n",
    "        pretrained_3 = 'fitting_classifier/checkpoints_densenet_test/test_clean/model_best_c4_0.849776_0.022063_1519323.pt'\n",
    "        \n",
    "        pretrained = '/blanca/project/wip/pix2pixHDX_class-master/models/fitting_classifier/checkpoints_densenet_test/test_clean/model_best_c4_0.849776_0.022063_1519323.pt'\n",
    "        for param in model.parameters(): param.requires_grad = False\n",
    "        \n",
    "        # uncomment below if  want to add pretrained   \n",
    "        checkpoint = pretrained\n",
    "        if checkpoint: model.load_state_dict(torch.load(checkpoint + 'h.tar'))  \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
